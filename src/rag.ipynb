{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "file_path = \"../data/data.pdf\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "\n",
    "        text = \"\"\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            try:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            except Exception:\n",
    "                print(f\"‚ö†Ô∏è Extract failed on page {i}\")\n",
    "\n",
    "    print(\"PDF loaded successfully!\")\n",
    "    print(text[:200])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå File not found:\", file_path)\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Unexpected error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Clean text\n",
    "- Remove unnecessary blank lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if line != \"\":\n",
    "            cleaned_lines.append(line)\n",
    "        elif len(cleaned_lines) > 0 and cleaned_lines[-1] != \"\":\n",
    "            cleaned_lines.append(\"\")\n",
    "    \n",
    "    cleaned = \"\\n\".join(cleaned_lines)\n",
    "    cleaned = re.sub(r\" {2,}\", \" \", cleaned)\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(text)\n",
    "print(cleaned_text[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=950,\n",
    "    chunk_overlap=130\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(cleaned_text)\n",
    "print(f\"Total chunks: {len(chunks)}\\n\")\n",
    "print(chunks[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### testing vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectordb.similarity_search(\"good students\")\n",
    "for r in results:\n",
    "    print(r.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Creating RAG model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "- Create retrieve search closely 3 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(\n",
    "    search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "- select LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "- create system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Use ONLY the information provided in the context to answer the question.\n",
    "Extract relevant details and summarize clearly. Do not invent any information.\n",
    "\n",
    "If the answer cannot be found in the context, reply:\n",
    "\"No information\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain(question: str, show_context=True):\n",
    "    # üîç Retrieve docs matched to question\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    # üß© Format prompt\n",
    "    formatted_prompt = prompt.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "\n",
    "    # ü§ñ LLM generate answer\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    answer = response.content\n",
    "\n",
    "    # üñ® Display nicely\n",
    "    print(\"üìå Input Question:\")\n",
    "    print(question)\n",
    "    print(\"\\nüìö Retrieved Context:\")\n",
    "    if show_context:\n",
    "        print(context)\n",
    "    else:\n",
    "        print(\"(hidden)\")\n",
    "    print(\"\\nüß† Model Output:\")\n",
    "    print(answer)\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = rag_chain(\"How many credits are there per semester?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = rag_chain(\"What are the grading criteria?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = rag_chain(\"Is my friend named Jimmy studying here?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = rag_chain(\"How to be a good student?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
